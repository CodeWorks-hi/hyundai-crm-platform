# +---------+
# | ê³ ê° ì„¼í„°  - Google Gemma-2-9B-IT ëª¨ë¸ API í˜¸ì¶œ|
# +---------+


import json
import re
import os
import time
import streamlit as st
import requests
from huggingface_hub import InferenceClient

TEXT_MODEL_ID = "google/gemma-2-9b-it"

# ğŸ“Œ Hugging Face API í† í° ê°€ì ¸ì˜¤ê¸° (`secrets.toml`ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°)


def get_huggingface_token(model_type):
    tokens = {"gemma": st.secrets.get("HUGGINGFACE_API_TOKEN_GEMMA")}
    return tokens.get(model_type)

def clean_input(text: str) -> str:
    return re.sub(r"(í•´ì¤˜|ì•Œë ¤ì¤˜|ì„¤ëª…í•´ ì¤˜|ë§í•´ ì¤˜)", "", text).strip()

def clean_html_tags(text):
    return re.sub(r'<[^>]+>', '', text)

def remove_unwanted_phrases(text: str) -> str:
    """
    ìƒì„±ëœ ê²°ê³¼ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ë¬¸êµ¬(ì˜ˆ: '[ê¸°íƒ€]', 'ìœ„ ë‚´ìš©ì€ ì§ˆë¬¸ì— ëŒ€í•œ', 
    'ë³´ê³ ì„œ ì‘ì„±ì„ ìœ„í•´ í•„ìš”í•œ ì •ë³´ëŠ” ë¬´ì—‡ì¸ì§€ìš”?' ë“±)ë¥¼ í¬í•¨í•œ ì¤„ì„ ì œê±°
    """
    lines = text.splitlines()
    filtered_lines = []
    for line in lines:
        if "[ê¸°íƒ€]" in line:
            continue
        if "ìœ„ ë‚´ìš©ì€ ì§ˆë¬¸ì— ëŒ€í•œ" in line:
            continue
        # ì¶”ê°€: ë³´ê³ ì„œ ì‘ì„±ì„ ìœ„í•œ ë¬¸êµ¬ ì œê±°
        if "ë³´ê³ ì„œ ì‘ì„±ì„ ìœ„í•´ í•„ìš”í•œ ì •ë³´ëŠ” ë¬´ì—‡ì¸ì§€ìš”?" in line:
            continue
        
        filtered_lines.append(line)
    
    return "\n".join(filtered_lines)

def format_predictions_for_api(predictions):
    if not predictions:
        return {}
    
# ğŸ“Œ ì‚¬ìš©ì ì…ë ¥ ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°)
def clean_input(text: str) -> str:
    return re.sub(r"\b(í•´ì¤˜|ì•Œë ¤ì¤˜|ì„¤ëª…í•´ ì¤˜|ì¶”ì²œí•´ ì¤˜|ë§í•´ ì¤˜)\b", "", text, flags=re.IGNORECASE).strip()

def generate_text_via_api(prompt: str, predictions: dict, news_items: list, model_name: str = TEXT_MODEL_ID) -> str:
    token = get_huggingface_token("gemma")
    if not token:
        st.error("Hugging Face API í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")
        return ""

    predictions_formatted = format_predictions_for_api(predictions)
    
    system_prompt = """
    [ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­]
    ### 1. ë¶„ì„ ìš”êµ¬ì‚¬í•­
    - í˜„ëŒ€/ê¸°ì•„ ê¸€ë¡œë²Œ íŒë§¤ ì „ëµ ë¶„ì„
    - ì˜ˆì¸¡ ë°ì´í„°ì™€ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•œ ë¶„ì„
    - ì§€ì—­ë³„(ë¶ë¯¸, ìœ ëŸ½, ì•„ì‹œì•„) íŒë§¤ ì „ëµ êµ¬ë¶„ ì„¤ëª…
    - í™˜ìœ¨ ë³€ë™ì´ ìˆ˜ì¶œ ì „ëµì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„
    - ê²½ì œ ìƒí™©ì— ë”°ë¥¸ ê¸ì •ì /ë¶€ì •ì  ìš”ì¸ êµ¬ë¶„
    - 3ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤(ë‚™ê´€/ì¤‘ë¦½/ë¹„ê´€)ë¡œ íŒë§¤ëŸ‰ ì˜ˆì¸¡

    ### 2. ì¶œë ¥ í˜•ì‹
    ## 2025 í˜„ëŒ€/ê¸°ì•„ ê¸€ë¡œë²Œ ì‹œì¥ ì „ë§ ë³´ê³ ì„œ
    | êµ¬ë¶„ | 2024 | 2025ì˜ˆìƒ | ì¦ê°ë¥  |
    |------|------|----------|--------|
    | ê¸€ë¡œë²Œ íŒë§¤ëŸ‰ | Xë§Œ ëŒ€ | Yë§Œ ëŒ€ | Z% |
    | ì£¼ìš” ì‹œì¥ ì ìœ ìœ¨ | A% | B% | C%p |

    - ì£¼ìš” ì „ëµ:
    - ë¦¬ìŠ¤í¬ ìš”ì¸:
    """

    full_prompt = f"{system_prompt}\n\n[ì˜ˆì¸¡ ë°ì´í„°]\n{predictions_formatted}\n\n[ì‚¬ìš©ì ì§ˆë¬¸]\n{prompt}"
    
    try:
        client = InferenceClient(model=model_name, token=token)
        response = client.text_generation(
            prompt=f"ë‹¤ìŒ ìš”ì²­ì— ë§ëŠ” ë¶„ì„ì„ ì „ë¬¸ê°€ì²˜ëŸ¼ 1000ì ë‚´ì™¸ë¡œ ìš”ì•½í•´ì¤˜:\n{full_prompt}",
            max_new_tokens=1000,
            temperature=0.7
        )
        return response
    except Exception as e:
        st.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return ""

    
def gemma_ui():
    st.title("AI ê¸°ë°˜ ì‹œì¥ ì˜ˆì¸¡ ë° ë¶„ì„")
    st.markdown("""
    - ì˜ˆì¸¡ ë°ì´í„°ì™€ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì¢…í•©í•œ ì‹¬ì¸µ ë¶„ì„ ì œê³µ
    - ìµœì‹  AI ê¸°ìˆ ì„ í™œìš©í•œ ì‹œì¥ ë™í–¥ ì˜ˆì¸¡
    - ë°ì´í„° ê¸°ë°˜ì˜ ê°ê´€ì ì´ê³  í†µì°°ë ¥ ìˆëŠ” ê²°ê³¼ ë„ì¶œ
    """)

    if 'predictions' not in st.session_state:
        st.warning("ë¨¼ì € 'ì˜ˆì¸¡ ì‹œìŠ¤í…œ' íƒ­ì—ì„œ ì˜ˆì¸¡ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return