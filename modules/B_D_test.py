import streamlit as st
import pandas as pd
from huggingface_hub import InferenceClient
import os

TEXT_MODEL_ID = "google/gemma-2-9b-it"
LOG_PATH = "data/qna_log.csv"

# Hugging Face API í† í° ë¶ˆëŸ¬ì˜¤ê¸°
def get_huggingface_token(model_type):
    tokens = {"gemma": st.secrets.get("HUGGINGFACE_API_TOKEN_GEMMA")}
    return 'hf_' + tokens.get(model_type)

# ë‹µë³€ ìƒì„± í•¨ìˆ˜
def get_car_info_based_on_question(user_input: str, model_name: str = TEXT_MODEL_ID) -> str:
    token = get_huggingface_token("gemma")
    if not token:
        return "âŒ Hugging Face API í† í°ì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."

    prompt = f"""
    ë‹¹ì‹ ì€ í˜„ëŒ€ìë™ì°¨ ì°¨ëŸ‰ ì¶”ì²œ ë° ì°¨ëŸ‰ ì •ë³´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ë©´ì„œë„ êµ¬ì²´ì ì¸ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”.
    ì§ˆë¬¸: {user_input.strip()}
    ë‹µë³€:
    """

    try:
        client = InferenceClient(model=model_name, token=token)
        response = client.text_generation(
            prompt=prompt,
            max_new_tokens=512,
            temperature=0.5,
        )
        return response.strip()
    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {e}"

def extract_tags_from_answer(answer: str) -> str:
    tag_prompt = f"""
    ë‹¤ìŒ ì„¤ëª…ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„ëœ íƒœê·¸ í˜•íƒœë¡œ ë½‘ì•„ì£¼ì„¸ìš”. ë‹¨ì–´ ìˆ˜ëŠ” 1~2ë‹¨ì–´ë¡œ ê°„ê²°í•˜ê²Œ í•´ì£¼ì„¸ìš”.

    ì„¤ëª…: {answer.strip()}
    íƒœê·¸:
    """
    token = get_huggingface_token("gemma")
    try:
        client = InferenceClient(model=TEXT_MODEL_ID, token=token)
        response = client.text_generation(
            prompt=tag_prompt,
            max_new_tokens=30,
            temperature=0.3,
        )
        return response.strip()
    except Exception as e:
        return "ì¼ë°˜"

# CSV ì €ì¥ í•¨ìˆ˜
def save_to_csv(question: str, answer: str):
    tag_str = extract_tags_from_answer(answer)

    new_row = pd.DataFrame([{
        "ì§ˆë¬¸": question,
        "ë‹µë³€": answer,
        "ë‹µë³€ íƒœê·¸": tag_str,
        "ë‚ ì§œ": pd.Timestamp.now().strftime("%Y-%m-%d"),
        "ì‹œê°„": pd.Timestamp.now().strftime("%H:%M:%S")
    }])

    os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)

    if os.path.exists(LOG_PATH):
        df = pd.read_csv(LOG_PATH)
        df = pd.concat([df, new_row], ignore_index=True)
    else:
        df = new_row

    df.to_csv(LOG_PATH, index=False)

# Streamlit í˜ì´ì§€ UI
def app():
    st.title("ğŸ” ì°¨ëŸ‰ ê´€ë ¨ AI ì§ˆë¬¸ ë„ìš°ë¯¸")

    user_question = st.text_area("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ì•„ì´ì˜¤ë‹‰5ì™€ ì•„ì´ì˜¤ë‹‰6 ì°¨ì´ì ì€?", height=150)

    if st.button("ë‹µë³€ ìƒì„±"):
        if user_question.strip():
            st.info("Gemma ëª¨ë¸ì´ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")
            result = get_car_info_based_on_question(user_question)
            st.session_state.generated_answer = result
            st.session_state.current_question = user_question
            st.session_state.generated_tags = extract_tags_from_answer(result)
        else:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    if "generated_answer" in st.session_state and st.session_state.generated_answer:
        st.success("AI ë‹µë³€ ê²°ê³¼")
        st.markdown(st.session_state.generated_answer)

        st.markdown("**íƒœê·¸:** " + st.session_state.generated_tags)

        if "ì˜¤ë¥˜ ë°œìƒ" not in st.session_state.generated_answer:
            save_to_csv(st.session_state.current_question, st.session_state.generated_answer)
            st.success("ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ìë™ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.warning("ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    st.markdown("---")
    st.subheader("ğŸ“‚ ì´ì „ ì§ˆë¬¸ ë° ë‹µë³€ ê¸°ë¡")

    if os.path.exists(LOG_PATH):
        log_df = pd.read_csv(LOG_PATH)
        st.dataframe(log_df, use_container_width=True)
    else:
        st.info("ì•„ì§ ì €ì¥ëœ ì§ˆë¬¸/ë‹µë³€ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")